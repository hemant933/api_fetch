x-airflow-common: &airflow-common
  image: apache/airflow:2.8.1
  env_file:
    - .env
  volumes:
    - ./dags:/opt/airflow/dags
    - ./spark_jobs:/opt/airflow/spark_jobs
    - ./requirements.txt:/requirements.txt
  user: "${AIRFLOW_UID}:${AIRFLOW_GID}"
  depends_on:
    mysql:
      condition: service_healthy   # wait until mysql is healthy

services:
  mysql:
    image: mysql:8.0
    container_name: mysql_airflow
    environment:
      MYSQL_ROOT_PASSWORD: root
      MYSQL_DATABASE: ${MYSQL_DB}
      MYSQL_USER: ${MYSQL_USER}
      MYSQL_PASSWORD: ${MYSQL_PASSWORD}
    ports:
      - "3306:3306"
    volumes:
      - ./mysql_data:/var/lib/mysql
    healthcheck:   # ensures MySQL is ready before Airflow runs
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost"]
      interval: 10s
      timeout: 5s
      retries: 5

  airflow-init:
    <<: *airflow-common
    entrypoint: /bin/bash
    command:
      - -c
      - |
        pip install --no-cache-dir -r /requirements.txt
        airflow db init
        airflow users create \
          --username ${_AIRFLOW_WWW_USER_USERNAME} \
          --firstname Admin --lastname User \
          --role Admin \
          --email admin@example.com \
          --password ${_AIRFLOW_WWW_USER_PASSWORD}
    depends_on:
      mysql:
        condition: service_healthy   #  ensure MySQL is ready

  airflow-webserver:
    <<: *airflow-common
    command: webserver
    ports:
      - "8080:8080"
    depends_on:
      mysql:
        condition: service_healthy

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    depends_on:
      mysql:
        condition: service_healthy

  spark-master:
    image: bitnami/spark:3.5.0
    container_name: spark-master
    environment:
      - SPARK_MODE=master
    ports:
      - "7077:7077"
      - "8081:8080"
    volumes:
      - ./spark_jobs:/opt/spark_jobs

  spark-worker:
    image: bitnami/spark:3.5.0
    container_name: spark-worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    depends_on:
      - spark-master
    volumes:
      - ./spark_jobs:/opt/spark_jobs